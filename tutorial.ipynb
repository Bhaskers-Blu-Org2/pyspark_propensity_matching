{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Propensity Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome \n",
    "Welcome to the PySpark Propensity Matching Tutorial Notebook  \n",
    "The goal of this library to enable the user to perform [propensity matching](https://en.wikipedia.org/wiki/Propensity_score_matching) on spark sized datasets. Propensity matching is typically performed to assess the impact of a treatment where the treatment is not randomly assigned. It tries to create a control population accounting for the other covariates and then assess the impact of the treatment on the response. \n",
    "\n",
    "\n",
    "### Tutorial\n",
    "The tutorial consists of the following parts:  \n",
    "    1. Data Generation  \n",
    "    2. Fitting the estimator  \n",
    "    3. Transforming the dataset  \n",
    "    4. Assessing the impact of our treatment  \n",
    "    5. Evaluating our performance  \n",
    "\n",
    "\n",
    "### Help\n",
    "For help with your specific use case, or any questions - please feel free to post to the [git repo](https://github.com/Microsoft/pyspark_propensity_matching)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.ml.feature as mlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[2]').getOrCreate()\n",
    "logging.getLogger('py4j').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(a=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a dataset using sklearn's excellent function *make_classification*. This allows us to create different features w/ different distributions for each treatment class & simulate the effect of self selection. \n",
    "Then we will model our response variable as a linear function of the features and the treatment class. We will compare our observed impact with the true impact dictated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    args = {\n",
    "        \"n_samples\": 10000,\n",
    "        \"n_features\": 20,\n",
    "        \"n_informative\": 10,\n",
    "        #\"n_redundant\": \n",
    "        #\"n_repeated\": \n",
    "        \"n_classes\": 2,\n",
    "        \"weights\": [.9, .1],\n",
    "        \"flip_y\": 0,\n",
    "        #\"class_sep\":  ,\n",
    "        #\"hypercube\":  ,\n",
    "        #\"shift\":  ,\n",
    "        #\"scale\":  ,\n",
    "        #\"shuffle\":  ,\n",
    "        \"random_state\": random.randint(0,100)\n",
    "    }\n",
    "\n",
    "    data, labels = make_classification(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe\n",
    "labels.shape=(10000,1)\n",
    "np_df = np.concatenate([data,labels], axis=1)\n",
    "\n",
    "cols = [\"f{0}\".format(x+1) for x in range(20)]\n",
    "cols.append('label')\n",
    "\n",
    "pd_df = pd.DataFrame(data=np_df, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create col w/ prob of being in positive response class\n",
    "coeffs = [random.random()/10 for x in range(args['n_features']+1)]\n",
    "def get_prob(x):\n",
    "    return 1/(1 + exp(-1*np.dot(x,coeffs)))\n",
    "pd_df['response_prob'] = pd_df[cols].apply(get_prob, axis=1)\n",
    "\n",
    "# assign class based on prob in response positive class col\n",
    "def get_class(x):\n",
    "    return int(x<random.random())\n",
    "pd_df['response'] = pd_df['response_prob'].apply(get_class)\n",
    "pd_df = pd_df.drop('response_prob', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the impact of our treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
